{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b533ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import openai_secret_manager\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader \n",
    "import time\n",
    "import base64\n",
    "import pytesseract\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "openai.organization = \"org-Ot0t5Ngu8AWutC1NC9B3ieck\"\n",
    "openai.api_key = os.getenv(\"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\")\n",
    "\n",
    "API_KEY = \"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\"\n",
    "API_ENDPOINT = \"https://api.openai.com/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_completion(messages, model=\"gpt-4-1106-preview\", temperature=0, max_tokens=None):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"seed\":1\n",
    "    }\n",
    "\n",
    "    if max_tokens is not None:\n",
    "        data[\"max_tokens\"] = max_tokens\n",
    "\n",
    "    response = requests.post(API_ENDPOINT, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def convert_json(response_text):\n",
    "    new_ = ''\n",
    "    start = False\n",
    "    for i in response_text:\n",
    "        if i == '{':\n",
    "            start = True\n",
    "        if start:\n",
    "            new_ += i\n",
    "#     return new_\n",
    "    u = 0\n",
    "    for i in new_[::-1]:\n",
    "        if i == '}':\n",
    "            break\n",
    "        else:\n",
    "            u += 1\n",
    "    new_ = new_[:-u]\n",
    "    new_ = new_.replace(' None','\"None\"')\n",
    "    return json.loads(new_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "filename = 'WASSMER PROTOCOL 2024.pdf'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_pdf = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512db5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e671f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4abc7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_enhancement(img):\n",
    "    norm_img = np.zeros((img.shape[0], img.shape[1]))\n",
    "    img = cv2.normalize(img, norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    img = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 15)\n",
    "    img =  cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    img= cv2.erode(img, kernel, iterations = 1)\n",
    "    img =  cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) [1]\n",
    "    cv2.imwrite('enhanced.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29846a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "if on_pdf:\n",
    "    lst_contents = []\n",
    "    pages = convert_from_path(filename, 500,poppler_path=r'poppler-0.68.0\\bin')\n",
    "    for i, page in enumerate(pages):\n",
    "        page.save('temp.jpg')\n",
    "        image_path = \"temp.jpg\"\n",
    "#         page = run_enhancement(page)\n",
    "        text = pytesseract.image_to_string(Image.open('temp.jpg'))\n",
    "        print(text)\n",
    "        lst_contents.append(text)        \n",
    "    text = '\\n'.join(lst_contents)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if on_pdf == False:\n",
    "    text = getText(filename).replace('\\n\\n','\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f336b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text[:32000]\n",
    "text2 = text[32000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02049499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "using_full = True\n",
    "with_references = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def track_requests():\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv('number_requests.csv')\n",
    "        df['Date'] = df['Timestamp'].apply(lambda x: str(x).split(' ')[0])\n",
    "        date = str(datetime.datetime.now()).split(' ')[0]\n",
    "\n",
    "        df = df[df['Date'] == date]\n",
    "        lst_track = list(df['Number of Requests'])\n",
    "        lst_timestamp = list(df['Timestamp'])\n",
    "        if lst_track:\n",
    "            new_val = int(max(lst_track)) + 1\n",
    "        else:\n",
    "            new_val = 1\n",
    "        now = str(datetime.datetime.now())\n",
    "        lst_track.append(new_val)\n",
    "        lst_timestamp.append(now)\n",
    "        pd.DataFrame({'Timestamp':lst_timestamp, 'Number of Requests':lst_track}).to_csv('number_requests.csv')\n",
    "    except:\n",
    "        \n",
    "        lst_date = []\n",
    "        lst_track = []\n",
    "        lst_date.append(str(datetime.datetime.now()))\n",
    "        lst_track.append(1)\n",
    "        pd.DataFrame({'Timestamp':lst_date, 'Number of Requests':lst_track}).to_csv('number_requests.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2873363",
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_references:\n",
    "    dict_references = {}\n",
    "    for ii in os.listdir('References'):\n",
    "        time.sleep(20)\n",
    "        reader = PdfReader('References\\\\' + ii) \n",
    "        print(len(reader.pages)) \n",
    "        print()\n",
    "#         references = references + '\\n' + str(i)\n",
    "        text = ''\n",
    "        for i in range(len(reader.pages)):\n",
    "            page = reader.pages[i] \n",
    "            text = text + ' ' + page.extract_text() \n",
    "    #         print(len(text))  \n",
    "\n",
    "\n",
    "\n",
    "        message_content = \"comprehensively summarize retaining the important concepts into two paragraphs the article:  \" + text \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": message_content}\n",
    "        ]\n",
    "\n",
    "\n",
    "        track_requests()\n",
    "        response_text = generate_chat_completion(messages)\n",
    "        dict_references['Reference: ' + ii] = response_text\n",
    "#         references = references + '\\n' + response_text + '\\n'\n",
    "        print(response_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31363736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# openai.api_key = \"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\"\n",
    "# Initialize the client\n",
    "client = openai.OpenAI(api_key = \"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\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Create an Assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Extract Information\",\n",
    "    instructions=\"Please extract the given information from text\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=\"gpt-4-1106-preview\"\n",
    ")\n",
    "\n",
    "# Step 2: Create a Thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Step 3: Add a Message to a Thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content='Please read the first half of the text: ' + text1 \n",
    ")\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content='Please read the second hald of the text: ' + text2  + ' . Concatenate it with the provided first half of the text' \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "\n",
    "    time.sleep(5)  \n",
    "    track_requests()\n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    print(run_status.model_dump_json(indent=4))\n",
    "\n",
    "    # If run is completed, get messages\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        i += 1\n",
    "        # Loop through messages and print content based on role\n",
    "\n",
    "        for msg in messages.data:\n",
    "            print(msg.content[0].text.value)\n",
    "            print('-------------------')\n",
    "            role = msg.role\n",
    "            content = msg.content[0].text.value\n",
    "#             print(content)\n",
    "\n",
    "        if i == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75618077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    details = ''\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content='Please extract the following details: 1.) author, 2.) research title, 3.) cellphone number of the author, 4.) institution of the author, 5.) department of the author, 6.) email of the author, 7.) general objective 8.) specific objectives (Provide it in JSON format with the details as the keys  and put None if there are no values to extracted)'\n",
    "\n",
    "    ) \n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "    )\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        track_requests()\n",
    "        time.sleep(5)  \n",
    "\n",
    "        # Retrieve the run status\n",
    "        run_status = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        print(run_status.model_dump_json(indent=4))\n",
    "\n",
    "        # If run is completed, get messages\n",
    "        if run_status.status == 'completed':\n",
    "            messages = client.beta.threads.messages.list(\n",
    "                thread_id=thread.id\n",
    "            )\n",
    "\n",
    "            # Loop through messages and print content based on role\n",
    "\n",
    "            for msg in messages.data:\n",
    "                details = msg.content[0].text.value\n",
    "                print(msg.content[0].text.value)\n",
    "                print('-------------------')\n",
    "                role = msg.role\n",
    "                content = msg.content[0].text.value\n",
    "    #             print(content)\n",
    "                break\n",
    "\n",
    "            break\n",
    "\n",
    "    det = convert_json(details)\n",
    "\n",
    "except:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if using_full == True:\n",
    "    try:\n",
    "        text_contents = ''\n",
    "\n",
    "        message_content = \"Please provide the fulll contents  from the text. Please do not shorten, summarize, or lessen. Divide the Output into JSON Elements with the Section name as the key. Dont reword anything. Please do not include characters that are subscripts\"\n",
    "        message = client.beta.threads.messages.create(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=message_content\n",
    "        )\n",
    "\n",
    "        run = client.beta.threads.runs.create(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "        )\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "\n",
    "            time.sleep(5)  \n",
    "            track_requests()\n",
    "            # Retrieve the run status\n",
    "            run_status = client.beta.threads.runs.retrieve(\n",
    "                thread_id=thread.id,\n",
    "                run_id=run.id\n",
    "            )\n",
    "            print(run_status.model_dump_json(indent=4))\n",
    "\n",
    "            # If run is completed, get messages\n",
    "            if run_status.status == 'completed':\n",
    "                run_success = True\n",
    "                messages = client.beta.threads.messages.list(\n",
    "                    thread_id=thread.id\n",
    "                )\n",
    "\n",
    "                # Loop through messages and print content based on role\n",
    "\n",
    "                for msg in messages.data:\n",
    "                    print(msg.content[0].text.value)\n",
    "                    chapter1  = msg.content[0].text.value\n",
    "                    print('-------------------')\n",
    "                    role = msg.role\n",
    "                    text_contents = msg.content[0].text.value\n",
    "                    break\n",
    "        #             print(content)\n",
    "                break\n",
    "    #     chap1 = convert_json(chapter1)\n",
    "\n",
    "\n",
    "    except:\n",
    "        print('error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7755ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_contents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63823fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_contents= convert_json(text_contents)\n",
    "# dict_references\n",
    "if with_references:\n",
    "    for key in dict_references.keys():\n",
    "\n",
    "        dict_contents[key] = dict_references[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeee6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_key1 = []\n",
    "lst_val1 = []\n",
    "\n",
    "for u in [dict_contents]:\n",
    "# for u in [det,chap1,chap2,chap3]:\n",
    "    for i in u.keys():\n",
    "        if  isinstance(u[i], dict):\n",
    "            pass\n",
    "        else:\n",
    "            lst_key1.append(i)\n",
    "            lst_val1.append(u[i])\n",
    "\n",
    "lst_key = []\n",
    "lst_val = []\n",
    "\n",
    "for u in [dict_contents]:\n",
    "    for i in u.keys():\n",
    "        lst_key.append(i)\n",
    "        lst_val.append(u[i])\n",
    "df = pd.DataFrame({'Keys':lst_key, 'Values':lst_val})\n",
    "\n",
    "for i in df.iterrows():\n",
    "    initial_key = i[1][0]\n",
    "    if isinstance(i[1][1], dict):\n",
    "        new_dict = i[1][1]\n",
    "        \n",
    "        for new_key in new_dict.keys():\n",
    "            lst_key1.append(initial_key + ' :' + new_key)\n",
    "            lst_val1.append(new_dict[new_key])\n",
    "            \n",
    "df1 = pd.DataFrame({'Keys':lst_key1, 'Values':lst_val1})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ce970",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_key1 = []\n",
    "lst_val1 = []\n",
    "\n",
    "\n",
    "for u in [det]:\n",
    "    for i in u.keys():\n",
    "        if  isinstance(u[i], dict):\n",
    "            pass\n",
    "        else:\n",
    "            lst_key1.append(i)\n",
    "            lst_val1.append(u[i])\n",
    "\n",
    "            \n",
    "df0 = pd.DataFrame({'Keys':lst_key1, 'Values':lst_val1})\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc71479",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_values = []\n",
    "once = False\n",
    "for i in df0.iterrows():\n",
    "    print(i[1][0], i[1][1])\n",
    "    key = i[1][0]\n",
    "    val = i[1][1]\n",
    "    \n",
    "    key1 = key.replace('_', ' ')\n",
    "    key1 = key1.strip()\n",
    "    key1 = key1.lower()\n",
    "    if isinstance(val, dict):\n",
    "        new_val = ''\n",
    "        for key2 in val.keys():\n",
    "            new_val += key2 + '\\n' + val[key2] + '\\n'\n",
    "    elif isinstance(val, list):\n",
    "        new_val = ''\n",
    "        for elem in val:\n",
    "            new_val += elem + '\\n' \n",
    "    else:\n",
    "        new_val = val\n",
    "        \n",
    "    if 'author' in key1:\n",
    "        if once == False:\n",
    "            author = new_val\n",
    "            list_values.append(('author',author))\n",
    "            once = True\n",
    "\n",
    "    if 'title' in key1:\n",
    "        research_title= new_val\n",
    "        list_values.append(('research_title',research_title))\n",
    "        \n",
    "    if 'cellphone' in key1:\n",
    "        cellphone_number = new_val\n",
    "        list_values.append(('cellphone',cellphone_number))\n",
    "        \n",
    "    if 'institution' in key1:\n",
    "        institution = new_val\n",
    "        list_values.append(('institution',institution))\n",
    "    \n",
    "    if 'department' in key1:\n",
    "        department = new_val\n",
    "        list_values.append(('department',department))\n",
    "        \n",
    "    if 'email' in key1:\n",
    "        email = new_val\n",
    "        list_values.append(('email',email))\n",
    "\n",
    "    if 'general objective' in key1:\n",
    "        general_objective = new_val\n",
    "        list_values.append(('general objective', general_objective))\n",
    "\n",
    "    if 'specific objective' in key1:\n",
    "        specific_objective = new_val\n",
    "        list_values.append(('specific_objective',specific_objective))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a267cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eddd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_key1 = []\n",
    "lst_val1 = []\n",
    "\n",
    "\n",
    "for u in [dict_contents]:\n",
    "    for i in u.keys():\n",
    "        if  isinstance(u[i], dict):\n",
    "            pass\n",
    "        else:\n",
    "            lst_key1.append(i)\n",
    "            lst_val1.append(u[i])\n",
    "\n",
    "lst_key = []\n",
    "lst_val = []\n",
    "\n",
    "for u in [dict_contents]:\n",
    "    for i in u.keys():\n",
    "        lst_key.append(i)\n",
    "        lst_val.append(u[i])\n",
    "df = pd.DataFrame({'Keys':lst_key, 'Values':lst_val})\n",
    "\n",
    "for i in df.iterrows():\n",
    "    initial_key = i[1][0]\n",
    "    if isinstance(i[1][1], dict):\n",
    "        new_dict = i[1][1]\n",
    "        \n",
    "        for new_key in new_dict.keys():\n",
    "            lst_key1.append(initial_key + ' :' + new_key)\n",
    "            lst_val1.append(new_dict[new_key])\n",
    "            \n",
    "df1 = pd.DataFrame({'Keys':lst_key1, 'Values':lst_val1})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df1.iterrows():\n",
    "#     print(i[1][0], i[1][1])\n",
    "    key = i[1][0]\n",
    "    val = i[1][1]\n",
    "    \n",
    "    key1 = key.lower()\n",
    "    key1 = key1.strip()\n",
    "    key1 = key1.replace('_',' ')\n",
    "\n",
    "    if 'background' in key1:\n",
    "        background = val\n",
    "        list_values.append(('background',background))\n",
    "        \n",
    "    elif 'introduction' in key1:\n",
    "        background = val\n",
    "        list_values.append(('background',background))\n",
    "        \n",
    "    if 'methodology' in key1:\n",
    "        methodology = val\n",
    "        list_values.append(('methodology',methodology))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ad68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_contents = []\n",
    "for file in [dict_contents]:\n",
    "    chap_ = ''\n",
    "    for i in file.keys():\n",
    "    #     print(chap3[i])\n",
    "        if  isinstance(file[i], dict):\n",
    "            new_dict = file[i]\n",
    "\n",
    "            for new_key in new_dict.keys():\n",
    "                \n",
    "                if  isinstance(new_dict[new_key], dict):\n",
    "                    for new_key1 in new_dict[new_key].keys():\n",
    "                        \n",
    "                        if  isinstance(new_dict[new_key][new_key1], dict):\n",
    "                            for new_key2 in new_dict[new_key][new_key1].keys():\n",
    "                                \n",
    "                                if isinstance(new_dict[new_key][new_key1][new_key2], dict):\n",
    "                                    \n",
    "                                    for new_key3 in new_dict[new_key][new_key1][new_key2].keys():\n",
    "                                        chap_ = chap_ + '\\n' + new_key1 + '\\n' + new_dict[new_key][new_key1][new_key2][new_key3] \n",
    "                                        \n",
    "                                        \n",
    "                                else:\n",
    "                                    \n",
    "                                    chap_ = chap_ + '\\n' + new_key1 + '\\n' + new_dict[new_key][new_key1][new_key2] \n",
    "                        elif isinstance(new_dict[new_key][new_key1],list):\n",
    "                                \n",
    "                            chap_ = chap_ + '\\n' + new_key1 + '\\n' + '\\n'.join(new_dict[new_key][new_key1])\n",
    "                                \n",
    "                        else:\n",
    "                            val = str(new_dict[new_key][new_key1])\n",
    "                            chap_ = chap_ + '\\n' + new_key1 + '\\n' + val\n",
    "                \n",
    "                elif isinstance(new_dict[new_key],list):\n",
    "                    print(new_dict[new_key])\n",
    "                    try:\n",
    "                        chap_ = chap_ + '\\n' + new_key + '\\n' + '\\n'.join(new_dict[new_key])\n",
    "                    except:\n",
    "                        str_ = ''\n",
    "                        print(new_dict[new_key])\n",
    "                        for dct12 in new_dict[new_key]:\n",
    "                            if isinstance(dct12,dict):\n",
    "                                for key in dct12.keys():\n",
    "                                    \n",
    "                                    str_ = str_ + ' ' + dct12[key]\n",
    "                        chap_ = chap_ + '\\n' + new_key + '\\n' + str_\n",
    "                else:\n",
    "                    val = str(new_dict[new_key])\n",
    "                    chap_ = chap_ + '\\n' + new_key + '\\n' + val\n",
    "                    \n",
    "        elif isinstance(file[i],list):\n",
    "            chap_ = chap_ + '\\n' + i  + '\\n' + '\\n'.join(file[i])    \n",
    "        \n",
    "        else:\n",
    "            chap_ = chap_ + '\\n' + i  + '\\n' + file[i] \n",
    "    lst_contents.append(chap_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b1141",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Chapters':lst_contents}).to_csv('lst_contents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27272e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contents = pd.read_csv('lst_contents.csv')\n",
    "lst_contents = list(df_contents['Chapters'])\n",
    "lst_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ccc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e7828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json(response_text):\n",
    "    new_ = ''\n",
    "    \n",
    "    index1 = response_text.find('{')\n",
    "    \n",
    "    c1 = response_text[index1:]\n",
    "    index2 = c1.rfind('}')\n",
    "    str_ = c1[:index2+1]\n",
    "\n",
    "    \n",
    "    str_ = str_.replace('\\n','')\n",
    "    str_ = str_.replace(' None','\"None\"')  \n",
    "    return json.loads(str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json2(str1):\n",
    "    lst_ = ['1.)','2.)','3.)','4.)','5.)','6.)','7.)','8.)','9.)','10.)']\n",
    "    dct_ = {}\n",
    "    for ii in range(0,len(lst_)-1):\n",
    "        try:\n",
    "\n",
    "            val = lst_[ii]\n",
    "            next_val = lst_[ii+1]\n",
    "\n",
    "            indx = str1.index(val)\n",
    "            nxt_indx = str1.index(next_val)\n",
    "\n",
    "            dct1 = convert_json(str1[indx:nxt_indx])\n",
    "\n",
    "            for key in dct1.keys():\n",
    "\n",
    "                for u in lst_:\n",
    "                    key = key.replace(u,'')\n",
    "                key = key.strip()\n",
    "                val = dct1[key]\n",
    "                val  = val.replace(',  \"','')\n",
    "                dct_[key] = val\n",
    "\n",
    "        except:\n",
    "\n",
    "            val = lst_[ii]\n",
    "            next_val = lst_[ii+1]\n",
    "\n",
    "            indx = str1.index(val)\n",
    "            nxt_indx = str1.index(next_val)\n",
    "\n",
    "            key = str1[indx:nxt_indx].split('\":')[0]\n",
    "            val = str1[indx:nxt_indx].split('\":')[1]\n",
    "            for u in lst_:\n",
    "                key = key.replace(u,'')\n",
    "            key = key.strip()\n",
    "            val  = val.replace(',  \"','')\n",
    "            dct_[key] = val\n",
    "    #     print(dct_)\n",
    "    return dct_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import openai_secret_manager\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# openai.organization = \"org-Ot0t5Ngu8AWutC1NC9B3ieck\"\n",
    "# openai.api_key = os.getenv(\"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\")\n",
    "\n",
    "API_KEY = \"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\"\n",
    "API_ENDPOINT = \"https://api.openai.com/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_completion(messages, model=\"gpt-4-1106-preview\", temperature=0, max_tokens=None):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"seed\":1\n",
    "    }\n",
    "\n",
    "    if max_tokens is not None:\n",
    "        data[\"max_tokens\"] = max_tokens\n",
    "\n",
    "    response = requests.post(API_ENDPOINT, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdefd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Provide the following details completely: \n",
    "\n",
    "1.) Proposed Introduction with 4 paragraphs (what is known, what is not yet known, how the study can contribute to the evidence gap, significance of the study)\n",
    "2.) Improved General Objective \n",
    "3.) Improved Specific Objectives\n",
    "4.) Improved Version of the Research Title \n",
    "5.) Generate Null Hypothesis\n",
    "6.) Generate Alternative Hypothesis\n",
    "7.)Ethical Consideration (Provide a one-sentence comprehensive summary of the provided text  to be used as an introduction for ethical considerations section)\n",
    "8.) Community Consideration(Provide a one-sentence on what the study aims to investigate in relation to community consideration)\n",
    "9.) methodology( with study design and setting), \n",
    "10.) operational definitions, \n",
    "11.)inclusion criteria,\n",
    "12.)exclusion criteria, \n",
    "13.) data sources and data collection and quality assurance, \n",
    "14.)statistical plan of analysis, \n",
    "15.)budget in philippine pesos,\n",
    "16.)timeline for approval spanning 2 months, \n",
    "17.)data collection of 3 months\n",
    "18.)manuscript preparation for 2 months\n",
    "\n",
    "(Provide it in JSON format with the details as the keys and put None if there are no values to extracted)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140cf532",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "message_content = prompt + '. Extract it from these ' + lst_contents[0] \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": message_content}\n",
    "]\n",
    "\n",
    "track_requests()\n",
    "response_text = generate_chat_completion(messages)\n",
    "\n",
    "values = convert_json(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f2992",
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in values.keys():\n",
    "    val = values[key]\n",
    "\n",
    "    if isinstance(val, dict):\n",
    "        new_val = ''\n",
    "        for key2 in val.keys():\n",
    "            if isinstance(val[key2],list):\n",
    "                \n",
    "                for elem in val[key2]:\n",
    "                    new_val += elem + ' '\n",
    "            else:\n",
    "                if isinstance(val[key2],dict):\n",
    "                    new_val += key2 +'\\n'\n",
    "                    for key3 in val[key2].keys():\n",
    "#                         print(val[key2][key3])\n",
    "                        new_val +=  str( val[key2][key3]) + '\\n'\n",
    "                else:\n",
    "                    new_val += key2 + '\\n' + val[key2] + '\\n'\n",
    "    else:\n",
    "        new_val = val\n",
    "    key1 = key.replace('_',' ')\n",
    "    key1 =  key1.lower()\n",
    "    key1 = key1.strip()\n",
    "    new_val = str(new_val)\n",
    "    new_val = new_val.replace('None','')\n",
    "    print('---------------------')\n",
    "    print(key1)\n",
    "    print(new_val)\n",
    "    print()\n",
    "    if 'methodology' in key1:\n",
    "                \n",
    "        methodology = new_val\n",
    "        list_values.append(('methodology',methodology))\n",
    "        \n",
    "#     if 'operational definition' in key1:\n",
    "        \n",
    "#         operational_definitions = new_val\n",
    "#         list_values.append(('operational_definitions',operational_definitions))\n",
    "        \n",
    "    if 'inclusion criteria' in key1 or 'inclusion_criteria' in key1:\n",
    "        inclusion_criteria = new_val\n",
    "        list_values.append(('inclusion_criteria',inclusion_criteria))\n",
    "        \n",
    "    if 'exclusion criteria' in key1:\n",
    "        exclusion_criteria = new_val\n",
    "        list_values.append(('exclusion_criteria',exclusion_criteria))\n",
    "        \n",
    "    if 'data sources' in key1 or 'data_sources' in key1:\n",
    "        data_sources_and_data_collection_and_quality_assurance = new_val\n",
    "        list_values.append(('data_sources',new_val))\n",
    "        \n",
    "    if 'statistical plan' in key1 or 'statistical_plan' in key1:\n",
    "        statistical_plan_of_analysis = new_val\n",
    "        list_values.append(('statistical_plan',new_val))\n",
    "        \n",
    "\n",
    "    if 'budget' in key1:\n",
    "        budget = new_val\n",
    "        list_values.append(('budget',budget))\n",
    "        \n",
    "    if 'timeline' in key1:\n",
    "        timeline = new_val\n",
    "        list_values.append(('timeline',timeline))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef135385",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494cd3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e45419",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in values.keys():\n",
    "    val = values[key]\n",
    "\n",
    "    if isinstance(val, dict):\n",
    "        new_val = ''\n",
    "        for key2 in val.keys():\n",
    "            new_val += key2 + '\\n' + str(val[key2]) + '\\n'\n",
    "    else:\n",
    "        new_val = val\n",
    "    key1 = key.replace('_',' ')\n",
    "    key1 =  key1.lower()\n",
    "    key1 = key1.strip()\n",
    "\n",
    "        \n",
    "    print(key1)\n",
    "    if 'specific objective' in key1 or 'specific_objective' in key1:\n",
    "        proposed_specific_objective = new_val\n",
    "        list_values.append(('proposed specific objective',new_val))\n",
    "        \n",
    "    if 'general objective' in key1 or 'general_objective' in key1:\n",
    "        proposed_general_objective = new_val\n",
    "        list_values.append(('proposed general objective',new_val))\n",
    "        \n",
    "    if 'introduction' in key1:\n",
    "        proposed_introduction = new_val\n",
    "        list_values.append(('proposed introduction',new_val))\n",
    "        \n",
    "    if 'null hypothesis' in key1 or 'null_hypothesis' in key1:\n",
    "        null_hypothesis = new_val\n",
    "        list_values.append(('null hypothesis',new_val))\n",
    "        \n",
    "    if 'alternative hypothesis' in key1 or 'alternative_hypothesis' in key1:\n",
    "        alternative_hypothesis = new_val\n",
    "        list_values.append(('alternative hypothesis',new_val))\n",
    "        \n",
    "    if 'ethical consideration' in key1 or 'ethical_considerations' in key1:\n",
    "        ethical_considerations = new_val\n",
    "        list_values.append(('ethical_considerations',new_val))\n",
    "        \n",
    "    if 'community consideration' in key1 or 'community_consideration' in key1:\n",
    "        print('yes')\n",
    "        community_considerations = new_val\n",
    "        list_values.append(('community_considerations',new_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in values.keys():\n",
    "    val = values[key]\n",
    "\n",
    "    if isinstance(val, dict):\n",
    "        new_val = ''\n",
    "        for key2 in val.keys():\n",
    "            new_val += key2 + '\\n' + str(val[key2]) + '\\n'\n",
    "    else:\n",
    "        new_val = val\n",
    "    key1 = key.replace('_',' ')\n",
    "    key1 =  key1.lower()\n",
    "    key1 = key1.strip()\n",
    "\n",
    "\n",
    "    if 'title' in key1:\n",
    "        proposed_title = new_val\n",
    "        list_values.append(('proposed title',new_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f142a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_references:\n",
    "    text_references = ''\n",
    "    for key in dict_references.keys():\n",
    "        text_references =  text_references + '\\n\\n' + key + '\\n\\n' + dict_references[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_references:\n",
    "    text = ' Title' + '\\n\\n' + proposed_title + '\\n\\n' + 'Introduction' + '\\n\\n' + background  +'\\n\\n' + 'General Objective' + '\\n\\n' + general_objective + '\\n\\n' + 'Specific Objective' + '\\n\\n' + specific_objective +'\\n\\n' + text_references\n",
    "else:\n",
    "    text = ' Title' + '\\n\\n' + proposed_title + '\\n\\n' + 'Introduction' + '\\n\\n' + background  +'\\n\\n' + 'General Objective' + '\\n\\n' + general_objective + '\\n\\n' + 'Specific Objective' + '\\n\\n' + specific_objective    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52645486",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = '''Given the title,background, general objective, specific objective of the research protocol, and some references, do the following:\n",
    "\n",
    "1.) Assess which Research Framework (PICO, PECO, PEO, PIRT, SPICE, etc.) fits the research protocol, \n",
    "then extract the specifics of the elements from the protocol.\n",
    "List the elements  and specifics of the research framework.  Put the results in dictionary format with key as the element and value as the specific\n",
    "\n",
    "2.) What is the research protocol's classification based on the below categories (choose one each classification only):\n",
    "\n",
    "- Temporality (Retrospective or Prospective?)\n",
    "\n",
    "- Level of Analysis (Descriptive or Analytical?)\n",
    "\n",
    "- Level of Intervention (Experimental or Observational?)\n",
    "\n",
    "- Specific study type (Cross-sectional, Cohort, Case-control, RCT, Quasi-experimental, Mixed method?)\n",
    "\n",
    "- Category (Clinical scoring, Diagnostics, Epidemiological, Health services, KAP, Product development, SRMA, Validation?\n",
    "  Put the result in a dictionary. Set the key as the category then value as the classification\n",
    "  \n",
    "3.) Extract all possible outcomes of interest in this research protocol: \n",
    "    Name of outcome, Metric/measurement of this outcome, Time Endpoints.\n",
    "    Put the results in a dictionary format with three keys: Name of outcome, Metric/Measurement and Time endpoints and values for the results of the three\n",
    "\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c25776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "message_content = text + '\\n' + question1 \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": message_content}\n",
    "]\n",
    "\n",
    "track_requests()\n",
    "response_text = generate_chat_completion(messages)\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d991dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = response_text.split('}')[0] + '}'\n",
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_ = convert_json(first)\n",
    "dct_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_elements = list(dct_.keys())\n",
    "lst_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_specifics = []\n",
    "for i in dct_.keys():\n",
    "    lst_specifics.append(dct_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c89f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "second = response_text.split('}')[1] + '}'\n",
    "second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_ = convert_json(second)\n",
    "dct_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403533fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lst_categories = list(dct_.keys())\n",
    "lst_classifications = []\n",
    "for i in dct_.keys():\n",
    "    lst_classifications.append(dct_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "third = response_text.split('}')[2] + '}'\n",
    "dct_outcome = convert_json(third)\n",
    "dct_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f926a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"Given the title,background, general objective, specific objective of the research protocol, and some references\" \n",
    "question2 = \"List down the Operational Definitions of the research protocol. Put the result in the first LIST and put '-------------' at the end of the LIST.\"\n",
    "question3 = \"List down the Procedures in this research protocol. Put the result in the second LIST and put '-------------' at the end of the LIST.\" \n",
    "question4 = \"List down the Exposures of the research protocol. Put the result in the third LIST and put '-------------' at the end of the LIST.\"\n",
    "question5 = \"List down the Confounders in this research protocol. Put the result in the fourth LIST and put '-------------' at the end of the LIST.\"\n",
    "question6 = \"List down the Effect Modifiers in this research protocol. Put the result in the fifth LIST and put '-------------' at the end of the LIST.\"\n",
    "question7 = \"Draft a paragraph outlining the appropriate sampling method and randomization, based on the research protocol, along with its study design, time endpoints, and institution, if applicable. If you deem it is not applicable however, please indicate as well. Put the results in the sixth LIST and put '-------------' at the end of the LIST.\"\n",
    "question8 = \"Draft a paragraph outlining the appropriate blinding method, based on the research protocol, along with its study design, time endpoints, and institution, if applicable. If you deem it is not applicable however, please indicate as well. Put the results in the seventh LIST and put '-------------' at the end of the LIST.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_content = text + '\\n' + question1 + '\\n' + question2 +'\\n' + question3+ '\\n' + question4 + '\\n' + question5 +'\\n' + question6+ '\\n' + question7 +'\\n' + question8\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": message_content}\n",
    "]\n",
    "\n",
    "track_requests()\n",
    "response_text = generate_chat_completion(messages)\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = response_text.lower()\n",
    "response_text.split('-------')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower1 = response_text.split('-------')[0].lower()\n",
    "if 'operational definitions:' in lower1:\n",
    "    index = lower1.find('operational definitions:')\n",
    "    first = response_text.split('-------')[0][index+len('Operational Definitions:'):]\n",
    "else:\n",
    "    index = lower1.find('operational definitions')\n",
    "    first = response_text.split('-------')[0][index+len('Operational Definitions'):]\n",
    "    \n",
    "# index1 = lower1.find('procedures')\n",
    "\n",
    "\n",
    "lower2 = first.lower()\n",
    "index2 = lower2.rfind('.')\n",
    "first = first[:index2+1]\n",
    "\n",
    "lst_ = []\n",
    "counter = 1\n",
    "for i in first.split('\\n'):\n",
    "    if i:\n",
    "        if i[0] == '-':\n",
    "            i = i[1:]\n",
    "        lst_.append( str(counter) + '. ' + i)\n",
    "        counter += 1\n",
    "first = '\\n'.join(lst_)\n",
    "print(first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabace9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower1 = response_text.split('-------')[1].lower()\n",
    "if 'procedures:' in lower1:\n",
    "    index = lower1.find('procedures:')\n",
    "    second = response_text.split('-------')[1][index+len('Procedures:'):]\n",
    "else:\n",
    "    index = lower1.find('procedures')\n",
    "    second = response_text.split('-------')[1][index+len('procedures'):]\n",
    "# print(lower1)\n",
    "lst_ = []\n",
    "counter = 1\n",
    "for i in second.split('\\n'):\n",
    "    if i:\n",
    "        if i[0] == '-':\n",
    "            i = i[1:]\n",
    "        lst_.append( str(counter) + '. ' + i)\n",
    "        counter += 1\n",
    "second = '\\n'.join(lst_)\n",
    "print(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower1 = response_text.split('-------')[2].lower()\n",
    "# print(lower1)\n",
    "print(response_text.split('-------')[2])\n",
    "if 'exposures:' in lower1:\n",
    "    index = lower1.find('exposures:')\n",
    "    exposures  = response_text.split('-------')[2][index+len('Exposures:'):]\n",
    "else:\n",
    "    \n",
    "    index = lower1.find('exposures')\n",
    "    exposures  = response_text.split('-------')[2][index+len('Exposures:'):]\n",
    "# print('---')\n",
    "# print(exposures)\n",
    "\n",
    "lst_ = []\n",
    "counter = 1\n",
    "for i in exposures.split('\\n'):\n",
    "    if i:\n",
    "        if i[0] == '-':\n",
    "            i = i[1:]\n",
    "        lst_.append( str(counter) + '. ' + i)\n",
    "        counter += 1\n",
    "exposures = '\\n'.join(lst_)\n",
    "# exposures\n",
    "print(exposures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db01012",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower1 = response_text.split('-------')[3].lower()\n",
    "# print(lower1)\n",
    "if 'confounders:' in lower1:\n",
    "    index = response_text.split('-------')[3].find('confounders:')\n",
    "    confounders  = lower1[index+len('Confounders:'):]\n",
    "else:\n",
    "    \n",
    "    index = response_text.split('-------')[3].find('confounders')\n",
    "    confounders  = lower1[index+len('Confounders'):]\n",
    "\n",
    "lst_ = []\n",
    "counter = 1\n",
    "for i in confounders.split('\\n'):\n",
    "    if i:\n",
    "        if i[0] == '-':\n",
    "            i = i[1:]\n",
    "        lst_.append( str(counter) + '. ' + i)\n",
    "        counter += 1\n",
    "confounders = '\\n'.join(lst_)\n",
    "confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower1 = response_text.split('-------')[4].lower()\n",
    "# print(lower1)\n",
    "if 'effect modifiers:' in lower1:\n",
    "    index = lower1.find('effect modifiers:')\n",
    "    effect_modifiers  = response_text.split('-------')[4][index+len('Effect Modifiers:'):]\n",
    "else:\n",
    "    \n",
    "    index = lower1.find('effect modifiers')\n",
    "    effect_modifiers   = response_text.split('-------')[4][index+len('Effect Modifiers'):]\n",
    "\n",
    "lst_ = []\n",
    "counter = 1\n",
    "for i in effect_modifiers .split('\\n'):\n",
    "    if i:\n",
    "        if i[0] == '-':\n",
    "            i = i[1:]\n",
    "        lst_.append( str(counter) + '. ' + i)\n",
    "        counter += 1\n",
    "effect_modifiers  = '\\n'.join(lst_)\n",
    "effect_modifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda9414",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text.split('-------')[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebe79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower1 = response_text.split('-------')[5].lower()\n",
    "# print(lower1)\n",
    "if 'sampling method and randomization:' in lower1:\n",
    "    index = lower1.find('sampling method and randomization:')\n",
    "\n",
    "    sampling_methods = response_text.split('-------')[5][index+len('Sampling Method and Randomization:'):]\n",
    "elif 'sampling method and randomization' in lower1:\n",
    "    \n",
    "    index = lower1.find('sampling method and randomization')\n",
    "\n",
    "    sampling_methods   = response_text.split('-------')[5][index+len('Sampling Method and Randomization'):]\n",
    "elif 'sampling method:' in lower1:\n",
    "    index = lower1.find('sampling method:')\n",
    "\n",
    "    sampling_methods = response_text.split('-------')[5][index+len('Sampling Method:'):]\n",
    "\n",
    "\n",
    "lst_ = []\n",
    "# counter = 1\n",
    "for i in sampling_methods.split('\\n'):\n",
    "    if i:\n",
    "        if i[0] == '-':\n",
    "            i = i[1:]\n",
    "        lst_.append( i)\n",
    "        counter += 1\n",
    "\n",
    "sampling_methods  = ' '.join(lst_)\n",
    "print(sampling_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb59879",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower1 = response_text.split('-------')[6].lower()\n",
    "# print(lower1)\n",
    "if 'blinding methods:' in lower1:\n",
    "    index = lower1.find('blinding methods:')\n",
    "    blinding  = response_text.split('-------')[6][index+len('Blinding Methods:'):]\n",
    "    \n",
    "elif 'blinding method:' in lower1:\n",
    "    index = lower1.find('blinding method:')\n",
    "    blinding  = response_text.split('-------')[6][index+len('Blinding Method:'):]\n",
    "elif 'blinding method' in lower1:\n",
    "    index = lower1.find('blinding method')\n",
    "    blinding  = response_text.split('-------')[6][index+len('Blinding Method'):]\n",
    "    \n",
    "\n",
    "lst_ = []\n",
    "counter = 1\n",
    "for i in blinding  .split('\\n'):\n",
    "    if i:\n",
    "        if i[0] == '-':\n",
    "            i = i[1:]\n",
    "        lst_.append( i)\n",
    "        counter += 1\n",
    "blinding   = ' '.join(lst_)\n",
    "blinding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = '''Given the title,background, general objective, specific objective of the research protocol, and some references, \n",
    "\n",
    "generate dummy tables and figures, collecting appropriate fields\n",
    "''' \n",
    "\n",
    "track_requests()\n",
    "message_content = text + '\\n' + question1 \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": message_content}\n",
    "]\n",
    "\n",
    "\n",
    "response_text = generate_chat_completion(messages)\n",
    "dummy_tables_figures = response_text\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_tables = {}\n",
    "num_cols = len(response_text.split('Table')[1])\n",
    "# print(num_cols)\n",
    "num_rows = 0\n",
    "for i in response_text.split('Table'):\n",
    "    lst_all = []\n",
    "    count_ = i.count('|')\n",
    "    if count_ > 5:\n",
    "        \n",
    "        index = i.find('|')\n",
    "        title = i[:index]\n",
    "        title = title.replace('**','')\n",
    "        title = title.replace('\\n','')\n",
    "#         print(title)\n",
    "        index1 = i.rfind('|')\n",
    "        table = i[index:index1+1]\n",
    "#         print(table)\n",
    "        \n",
    "        table = table.strip()\n",
    "        if table != '' and '|' in table:\n",
    "            lst_ = []\n",
    "            num_cols = len(table.split('|'))\n",
    "            num_rows += 1\n",
    "            for u in table.split('|')[1:]:\n",
    "                u = u.replace('-','')\n",
    "#                 u = u.strip()\n",
    "                if u:\n",
    "                    lst_.append(u)\n",
    "            if lst_:\n",
    "                str_ = '**'.join(lst_)\n",
    "#                 lst_all.append(str_)\n",
    "#         print('****'.join(lst_all))\n",
    "#         print('-----------------------------------------------')\n",
    "        dct_tables['Dummy Table: ' + title] = str_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dummy_tables_figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aeaafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = '''Based on the dummy tables and figures, create a data collection tool table''' \n",
    "\n",
    "\n",
    "message_content = dummy_tables_figures + '\\n' + question1 \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": message_content}\n",
    "]\n",
    "\n",
    "track_requests()\n",
    "response_text = generate_chat_completion(messages)\n",
    "data_collection_tool = response_text\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(data_collection_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collection_tool = response_text\n",
    "\n",
    "index = data_collection_tool .find('|')\n",
    "\n",
    "index1 = data_collection_tool .rfind('|')\n",
    "description  = data_collection_tool[index1+1:]\n",
    "description = description.replace('**','')\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collection_tool = response_text\n",
    "\n",
    "index = data_collection_tool .find('|')\n",
    "\n",
    "index1 = data_collection_tool .rfind('|')\n",
    "description  = data_collection_tool[index1+1:]\n",
    "description = description.replace('**','')\n",
    "\n",
    "index = data_collection_tool .find('|')\n",
    "\n",
    "index1 = data_collection_tool .rfind('|')\n",
    "table = data_collection_tool [index:index1+1]\n",
    "# print(table)\n",
    "lst_all = []\n",
    "for row in table.split('\\n'):\n",
    "    if 5 > row.count('-'):\n",
    "        lst_ = []\n",
    "        for cell in row.split('|'):\n",
    "            if cell:\n",
    "                lst_.append(cell)\n",
    "        lst_all.append('**'.join(lst_))\n",
    "data_collection_tool = '*****'.join(lst_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf716d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_v = []\n",
    "lst_k = []\n",
    "for i in list_values:\n",
    "    lst_k.append(i[0])\n",
    "    lst_v.append(i[1])\n",
    "for i,u in zip(lst_elements, lst_specifics):\n",
    "    lst_k.append('element: ' + i)\n",
    "    if isinstance(u, list):\n",
    "        u = ', '.join(u)\n",
    "        lst_v.append(u)\n",
    "    else:\n",
    "        lst_v.append(u)\n",
    "for i,u in zip(lst_categories, lst_classifications):\n",
    "    lst_k.append('category ' + i)\n",
    "    lst_v.append(u)\n",
    "    \n",
    "lst_k.append('operational_definitions')\n",
    "lst_v.append(first)\n",
    "lst_k.append('procedures')\n",
    "lst_v.append(second)\n",
    "lst_k.append('exposures')\n",
    "lst_v.append(exposures)\n",
    "lst_k.append('confounders')\n",
    "lst_v.append(confounders)\n",
    "lst_k.append('effect_modifiers')\n",
    "lst_v.append(effect_modifiers)\n",
    "lst_k.append('sampling methods')\n",
    "lst_v.append(sampling_methods)\n",
    "lst_k.append('blinding')\n",
    "lst_v.append(blinding)\n",
    "\n",
    "for key in dct_outcome.keys():\n",
    "    lst_k.append(key)\n",
    "    lst_v.append(dct_outcome[key])\n",
    "\n",
    "\n",
    "related_literature = ''\n",
    "i = 0\n",
    "for content in lst_contents[0].split('\\n'):\n",
    "    if len(content) > 100:\n",
    "        if i < 3:\n",
    "            related_literature = '\\n' + content + related_literature\n",
    "        i+=1\n",
    "lst_k.append('related_literature')\n",
    "lst_v.append(related_literature)\n",
    "# lst_k.append('ethical_considerations')\n",
    "# lst_v.append(ethical_considerations)\n",
    "# lst_k.append('community_consideration')\n",
    "# lst_v.append(community_consideration)\n",
    "\n",
    "for key in dct_tables.keys():\n",
    "    lst_k.append(key)\n",
    "    lst_v.append(dct_tables[key])\n",
    "lst_k.append('data collection table')\n",
    "lst_v.append(data_collection_tool)\n",
    "lst_k.append('data collection table description')\n",
    "lst_v.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0891651",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Keys':lst_k, 'Values':lst_v})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('list_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,u in zip(df['Keys'],df['Values']):\n",
    "    print('----------------------------------------')\n",
    "    print(i)\n",
    "    print()\n",
    "    print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['Keys']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Keys'] =='background']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cc955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cebffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
